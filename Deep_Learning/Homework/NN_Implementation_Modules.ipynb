{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module** is like a black box, which operates forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    You can refer to Module as an abstract class. You need to override all the methods in inherited classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__ (self):\n",
    "        \"\"\"\n",
    "        output = module.forward(input)    \n",
    "        gradInput = module.backward(input, gradOutput)\n",
    "        \"\"\"\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.train = True\n",
    "\n",
    "    def forward(self, inpt):\n",
    "        \"\"\"\n",
    "        Performs forward-propagation: takes an input, and computes the corresponding output \n",
    "        (see updateOutput method).\n",
    "        \"\"\"\n",
    "        return self.updateOutput(inpt)\n",
    "\n",
    "    def backward(self, inpt, gradOutput):\n",
    "        \"\"\"\n",
    "        Performs backpropagation step through the module, with respect to the given input.\n",
    "        \n",
    "        This includes \n",
    "         - computing a local gradient w.r.t. input (see updateGradInput),\n",
    "         - computing a gradient w.r.t. parameters to update parameters while optimizing (see accGradParameters)\n",
    "        \"\"\"\n",
    "        self.updateGradInput(inpt, gradOutput)\n",
    "        self.accGradParameters(inpt, gradOutput)\n",
    "        return self.gradInput\n",
    "    \n",
    "\n",
    "    def updateOutput(self, inpt):\n",
    "        \"\"\"\n",
    "        Stores and returns the output using the current parameter set of the class and input. \n",
    "        Used in the forward propagation.\n",
    "        \"\"\"\n",
    "        # just some easy case        \n",
    "        self.output = inpt \n",
    "        return self.output\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def updateGradInput(self, inpt, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own input. \n",
    "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
    "        \n",
    "        The shape of `gradInput` is always the same as the shape of `input`.\n",
    "        \n",
    "        Make sure to both store the gradients in `gradInput` field and return it.\n",
    "        \"\"\"\n",
    "        \n",
    "        # just some easy case\n",
    "        self.gradInput = gradOutput \n",
    "        return self.gradInput\n",
    "        \n",
    "        pass   \n",
    "    \n",
    "    def accGradParameters(self, inpt, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own parameters.\n",
    "        No need to override if module has no parameters (e.g. ReLU).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self): \n",
    "        \"\"\"\n",
    "        Zeroes `gradParams` variable if the module has params.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "        \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with gradients with respect to its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Sets training mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Sets evaluation mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define forward and backward propagation workflows here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "    Refer to Sequential as a container, containing all the modules (layers) sequentially.\n",
    "    Input is processed by each module (layer) in self.modules consecutively.\n",
    "    The resulting array is the Output. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        \"\"\"\n",
    "        Adds a module (layer) to the container.\n",
    "        \"\"\"\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def updateOutput(self, inpt):\n",
    "        \"\"\"\n",
    "        Main workflow of forward-propagation (this method is called in the forward method of base class):\n",
    "        \n",
    "            y_0    = module[0].forward(input)\n",
    "            y_1    = module[1].forward(y_0)\n",
    "            ...\n",
    "            output = module[n-1].forward(y_{n-2})   \n",
    "            \n",
    "        \"\"\"\n",
    "        self.y=[]   \n",
    "        \n",
    "        #TODO: implement forward pass described in docstring\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, inpt, gradOutput):\n",
    "        \"\"\"\n",
    "        Main workflow of backward-propagation:\n",
    "            \n",
    "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
    "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
    "            ...\n",
    "            g_1 = module[1].backward(y_0, g_2)   \n",
    "            gradInput = module[0].backward(input, g_1)   \n",
    "             \n",
    "        Each module takes the input seen during the forward pass (y_{i-1}) \n",
    "        and computes corresonding gradient (g_{i}).   \n",
    "        \"\"\"\n",
    "        y = self.y\n",
    "        g = []\n",
    "        \n",
    "        #TODO: implement backward pass described in docstring\n",
    "        \n",
    "        return self.gradInput\n",
    "      \n",
    "\n",
    "    def zeroGradParameters(self): \n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "    \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all gradients w.r.t parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    Performs pre-activation thus outputting weighted input.\n",
    "    \n",
    "    The module should work with batch input 2D input of shape (n_samples, n_features).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        #TODO: Initialize biases and weights from standard normal distribution\n",
    "        \n",
    "        #self.b = \n",
    "        #self.W = \n",
    "    \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, inpt):\n",
    "        \n",
    "        #TODO: calculate output of linear layer\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, inpt, gradOutput):\n",
    "        \n",
    "        self.gradInput = gradOutput.dot(self.W)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, inpt, gradOutput):\n",
    "        \n",
    "        self.gradW = gradOutput.T.dot(inpt)\n",
    "        self.gradb = gradOutput.sum(axis=0)\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, inpt):\n",
    "        self.output = (np.exp(inpt) / np.sum(np.exp(inpt), axis =1, keepdims = True))\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, inpt, gradOutput):\n",
    "        self.gradInput = self.output*(gradOutput - (np.sum(gradOutput*self.output, axis=1, keepdims = True)))\n",
    "        \n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "         super(Sigmoid, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, inpt):\n",
    "        \n",
    "        #TODO: implement sigmoid function\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, inpt, gradOutput):\n",
    "        \n",
    "        #TODO: implement gradient of sigmoid function\n",
    "        \n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Sigmoid\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "         super(Tanh, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, inpt):\n",
    "        \n",
    "        self.output = (1. - np.exp(-2*inpt)) / (1. + np.exp(-2*inpt))\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, inpt, gradOutput):\n",
    "        \n",
    "        self.gradInput = gradOutput*(1-self.output**2)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Tanh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, inpt):\n",
    "        \n",
    "        #TODO: implement RELU function\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, inpt, gradOutput):\n",
    "        \n",
    "        self.gradInput = gradOutput*(inpt>0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    \"\"\"\n",
    "    Base class for criterions.\n",
    "    \"\"\"\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "    \n",
    "    def forward(self, inpt, target):\n",
    "        \"\"\"\n",
    "        Given an input and a target, compute the loss function \n",
    "        associated to the criterion and return the result.\n",
    "            \n",
    "        (all the code goes in updateOutput)\n",
    "        \"\"\"\n",
    "        return self.updateOutput(inpt, target)\n",
    "\n",
    "    def backward(self, inpt, target):\n",
    "        \"\"\"\n",
    "        Given an input and a target, compute the gradients of the loss function\n",
    "        associated to the criterion and return the result. \n",
    "\n",
    "        (all the code goes in updateGradInput)\n",
    "        \"\"\"\n",
    "        return self.updateGradInput(inpt, target)\n",
    "    \n",
    "    def updateOutput(self, inpt, target):\n",
    "        \"\"\"\n",
    "        Override this.\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, inpt, target):\n",
    "        \"\"\"\n",
    "        Override this.\n",
    "        \"\"\"\n",
    "        return self.gradInput   \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MSECriterion** (L2 norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSECriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        super(MSECriterion, self).__init__()\n",
    "\n",
    "    def updateOutput(self, inpt, target):   \n",
    "        \n",
    "        #TODO: implement MSE\n",
    "        \n",
    "        return self.output \n",
    " \n",
    "    def updateGradInput(self, inpt, target):\n",
    "        \n",
    "        #TODO: calculate gradient w.r.t input\n",
    "\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CrossEntropyCriterion**. ([multiclass log loss](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy)). Remember that targets are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyCriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyCriterion, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, inpt, target): \n",
    "        \n",
    "        # trick to avoid numerical errors\n",
    "        input_clamp = np.maximum(1e-15, np.minimum(inpt, 1 - 1e-15) )\n",
    "\n",
    "        self.output = np.sum(target*np.log(input_clamp))/(-len(input_clamp))\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, inpt, target):\n",
    "        \n",
    "        input_clamp = np.maximum(1e-15, np.minimum(inpt, 1 - 1e-15))\n",
    "                \n",
    "        #TODO: calculate gradient w.r.t input\n",
    "        \n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"CrossEntropyCriterion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
